# ğŸ¤– Agentic AI Assistant

This is a multi-modal AI assistant that answers user queries using:
- ğŸ“„ PDF documents (local course material)
- ğŸŒ Web search via Tavily
- ğŸ” Observability using LangSmith
- ğŸ§  LLMs via Groq's LLaMA3 model
- ğŸ“š Vector search using FAISS + HuggingFace embeddings

---

## ğŸ› ï¸ Tech Stack
- Python
- LangChain
- FastAPI (Backend)
- Streamlit (Frontend)
- FAISS + HuggingFace (Embeddings)
- Tavily (Web Search)
- LangSmith (Tracing/Observability)
- Groq LLaMA3 (LLM provider)

---

## ğŸ“ Project Structure

```
final_project/
â”‚
â”œâ”€â”€ agent.py        # Main agent logic with tools
â”œâ”€â”€ app.py          # Streamlit frontend
â”œâ”€â”€ main.py         # FastAPI backend
â”œâ”€â”€ rag.py
â”œâ”€â”€ utils.py
â”œâ”€â”€ api.py          # Supporting code
â”œâ”€â”€ vectorstore/    # Saved vector DB (FAISS)
â”œâ”€â”€ data/           # Raw PDF files
â”œâ”€â”€ .env            # API keys (not shared)
â””â”€â”€ README.md       # This file
```

---

## ğŸš€ How to Run Locally

### 1. Clone the project

```bash
git clone <your-repo-url>
cd final_project
```

### 2. Add your `.env` file

Create a `.env` file in the root and add:

```ini
GROQ_API_KEY=your_key
LANGSMITH_API_KEY=your_key
TAVILY_API_KEY=your_key
```

### 3. Install dependencies

```bash
pip install -r requirements.txt
```

### 4. Run the FastAPI backend

```bash
uvicorn main:app --reload
```

### 5. In a second terminal, run the Streamlit frontend

```bash
streamlit run app.py
```

---

## ğŸ’¬ Sample Q&A

> **Q:** What is importance of rheology?  
> **A:** Rheology provides a foundational understanding of material behavior and is critical for modeling biological structures and improving industrial processes.

---

## âœ… Submission Ready

- âœ”ï¸ Fully working: PDF + Web + Agent + LangSmith  
- âœ”ï¸ Frontend + Backend connected  
- âœ”ï¸ Ready to submit on GitHub and Google Classroom
